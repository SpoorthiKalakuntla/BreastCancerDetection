{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da909aa-cd52-44e5-b64e-c48d00f74462",
   "metadata": {},
   "source": [
    "D:\\BreastCancer\\preprocessed_BUSI_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4066daca-0a82-4cd9-a5d2-4f828d7ff393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50  # Replace VGG16 with ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "ultrasound_dir = r'D:\\BreastCancer\\preprocessed_BUSI_data'\n",
    "\n",
    "# Image size for ResNet50\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224  # ResNet50 expects 224x224 images\n",
    "\n",
    "# Helper function to load images from a directory\n",
    "def load_images_from_folder(folder):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for label_value, label_name in enumerate(['no', 'yes']):\n",
    "        label_folder = os.path.join(folder, label_name)\n",
    "        for image_name in os.listdir(label_folder):\n",
    "            if image_name.endswith('.png') or image_name.endswith('.jpg'):\n",
    "                image_path = os.path.join(label_folder, image_name)\n",
    "                image = tf.keras.preprocessing.image.load_img(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "                image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "                dataset.append(image_array)\n",
    "                labels.append(label_value)\n",
    "    return np.array(dataset), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f125c6a4-8683-4ad6-868d-caa6dcbd4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ultrasound_data, ultrasound_labels = load_images_from_folder(ultrasound_dir)\n",
    "# Split dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(ultrasound_data, ultrasound_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Normalize images\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc0f433-dae2-43c4-9be2-9d7d1efb82fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CVR\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\CVR\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load pre-trained SqueezeNet\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "squeezenet = models.squeezenet1_1(pretrained=True)\n",
    "\n",
    "# Freeze all layers (Optional: If doing Transfer Learning)\n",
    "for param in squeezenet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the last classifier layer for 2 classes (No Cancer, Cancer)\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "squeezenet.num_classes = 2\n",
    "squeezenet = squeezenet.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(squeezenet.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838733ec-2b46-4f48-b158-c4f86a58ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class (if using Image Dataset)\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11e0b3be-f895-4dde-96cc-64b9e61184d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.5899, Train Acc: 0.6759 | Val Loss: 0.5403, Val Acc: 0.7110\n",
      "Epoch 2/20 | Train Loss: 0.4855, Train Acc: 0.7760 | Val Loss: 0.4537, Val Acc: 0.7871\n",
      "Epoch 3/20 | Train Loss: 0.4242, Train Acc: 0.8055 | Val Loss: 0.4010, Val Acc: 0.8213\n",
      "Epoch 4/20 | Train Loss: 0.3863, Train Acc: 0.8275 | Val Loss: 0.3689, Val Acc: 0.8289\n",
      "Epoch 5/20 | Train Loss: 0.3531, Train Acc: 0.8465 | Val Loss: 0.3471, Val Acc: 0.8441\n",
      "Epoch 6/20 | Train Loss: 0.3333, Train Acc: 0.8551 | Val Loss: 0.3298, Val Acc: 0.8631\n",
      "Epoch 7/20 | Train Loss: 0.3198, Train Acc: 0.8732 | Val Loss: 0.3170, Val Acc: 0.8631\n",
      "Epoch 8/20 | Train Loss: 0.3007, Train Acc: 0.8856 | Val Loss: 0.3085, Val Acc: 0.8631\n",
      "Epoch 9/20 | Train Loss: 0.2948, Train Acc: 0.8732 | Val Loss: 0.2988, Val Acc: 0.8745\n",
      "Epoch 10/20 | Train Loss: 0.2897, Train Acc: 0.8866 | Val Loss: 0.2921, Val Acc: 0.8707\n",
      "Epoch 11/20 | Train Loss: 0.2796, Train Acc: 0.8875 | Val Loss: 0.2858, Val Acc: 0.8783\n",
      "Epoch 12/20 | Train Loss: 0.2677, Train Acc: 0.8980 | Val Loss: 0.2835, Val Acc: 0.8707\n",
      "Epoch 13/20 | Train Loss: 0.2650, Train Acc: 0.8923 | Val Loss: 0.2775, Val Acc: 0.8935\n",
      "Epoch 14/20 | Train Loss: 0.2643, Train Acc: 0.8904 | Val Loss: 0.2749, Val Acc: 0.8783\n",
      "Epoch 15/20 | Train Loss: 0.2540, Train Acc: 0.9018 | Val Loss: 0.2708, Val Acc: 0.8935\n",
      "Epoch 16/20 | Train Loss: 0.2552, Train Acc: 0.8970 | Val Loss: 0.2674, Val Acc: 0.8935\n",
      "Epoch 17/20 | Train Loss: 0.2512, Train Acc: 0.8970 | Val Loss: 0.2651, Val Acc: 0.9011\n",
      "Epoch 18/20 | Train Loss: 0.2423, Train Acc: 0.8999 | Val Loss: 0.2637, Val Acc: 0.8973\n",
      "Epoch 19/20 | Train Loss: 0.2428, Train Acc: 0.9104 | Val Loss: 0.2622, Val Acc: 0.8973\n",
      "Epoch 20/20 | Train Loss: 0.2374, Train Acc: 0.9028 | Val Loss: 0.2590, Val Acc: 0.8973\n",
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CancerDataset(x_train, y_train, transform=transform)\n",
    "test_dataset = CancerDataset(x_test, y_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "num_epochs = 20\n",
    "train_losses, val_losses = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    squeezenet.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    # Training Phase\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = squeezenet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Compute Training Accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, true_labels = torch.max(labels, 1)  # Convert one-hot to class index\n",
    "        correct += (predicted == true_labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_acc.append(train_accuracy)\n",
    "\n",
    "    # Validation Phase\n",
    "    squeezenet.eval()\n",
    "    val_running_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = squeezenet(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            # Compute Validation Accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, true_labels = torch.max(labels, 1)\n",
    "            val_correct += (predicted == true_labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss = val_running_loss / len(test_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_acc.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1fbf297-5da5-483c-acfd-c4c9985945b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "class DingoOptimizer:\n",
    "    def __init__(self, model_class, param_space, population_size=5, iterations=10, device='cpu'):\n",
    "        self.model_class = model_class\n",
    "        self.param_space = param_space\n",
    "        self.population_size = population_size\n",
    "        self.iterations = iterations\n",
    "        self.device = device\n",
    "        self.best_params = None\n",
    "        self.best_model = None\n",
    "\n",
    "    def sample_params(self):\n",
    "        return {k: random.choice(v) for k, v in self.param_space.items()}\n",
    "\n",
    "    def optimize(self, train_loader, val_loader):\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            print(f\"\\n--- Iteration {i+1}/{self.iterations} ---\")\n",
    "            candidates = [self.sample_params() for _ in range(self.population_size)]\n",
    "\n",
    "            for params in candidates:\n",
    "                model = self.model_class().to(self.device)\n",
    "                model.classifier[1] = nn.Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "                model.num_classes = 2\n",
    "\n",
    "                optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                val_loss = self.train_and_evaluate(model, optimizer, criterion, train_loader, val_loader, params['epochs'])\n",
    "\n",
    "                print(f\"Params: {params}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    self.best_params = params\n",
    "                    self.best_model = deepcopy(model)\n",
    "\n",
    "        return self.best_model, self.best_params\n",
    "\n",
    "    def train_and_evaluate(self, model, optimizer, criterion, train_loader, val_loader, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "                val_loss += loss.item()\n",
    "        return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cffc9fb-c00a-4bb8-aef0-b8e02b086e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'lr': [0.01, 0.001, 0.0001],\n",
    "    'epochs': [5, 10, 15]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e420aa26-772b-4ba1-8b74-3f5a3648d030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iteration 1/3 ---\n",
      "Params: {'lr': 0.0001, 'epochs': 10}, Val Loss: 0.3104\n",
      "Params: {'lr': 0.001, 'epochs': 15}, Val Loss: 0.2248\n",
      "Params: {'lr': 0.0001, 'epochs': 5}, Val Loss: 0.3760\n",
      "Params: {'lr': 0.01, 'epochs': 15}, Val Loss: 0.2193\n",
      "\n",
      "--- Iteration 2/3 ---\n",
      "Params: {'lr': 0.001, 'epochs': 15}, Val Loss: 0.2432\n",
      "Params: {'lr': 0.001, 'epochs': 15}, Val Loss: 0.2222\n",
      "Params: {'lr': 0.01, 'epochs': 10}, Val Loss: 0.2117\n",
      "Params: {'lr': 0.0001, 'epochs': 10}, Val Loss: 0.3040\n",
      "\n",
      "--- Iteration 3/3 ---\n",
      "Params: {'lr': 0.0001, 'epochs': 10}, Val Loss: 0.2881\n",
      "Params: {'lr': 0.0001, 'epochs': 10}, Val Loss: 0.2970\n",
      "Params: {'lr': 0.0001, 'epochs': 15}, Val Loss: 0.2537\n",
      "Params: {'lr': 0.01, 'epochs': 15}, Val Loss: 0.2125\n",
      "\n",
      "Best Hyperparameters: {'lr': 0.01, 'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "# Function to return a new SqueezeNet model\n",
    "def build_squeezenet():\n",
    "    model = models.squeezenet1_1(pretrained=True)\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "# Instantiate and run Dingo\n",
    "dingo = DingoOptimizer(\n",
    "    model_class=build_squeezenet,\n",
    "    param_space=param_space,\n",
    "    population_size=4,\n",
    "    iterations=3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "best_model, best_params = dingo.optimize(train_loader, test_loader)\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9710ac-99e9-4b23-bb0f-e5cc3e026259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_model():\n",
    "    model = models.squeezenet1_1(pretrained=True)\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.classifier[1] = nn.Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "    model.num_classes = 2\n",
    "    return model.to(device)\n",
    "\n",
    "# Use best parameters\n",
    "lr = best_params['lr']\n",
    "epochs = best_params['epochs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5dee086-3a49-4c56-8d62-5bac328448b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_final_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f886b342-f8f1-429a-8c11-54450799902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.4667, Acc: 0.6702 | Val Loss: 0.4258, Acc: 0.6654\n",
      "Epoch 2/10 | Train Loss: 0.3946, Acc: 0.6826 | Val Loss: 0.3971, Acc: 0.6654\n",
      "Epoch 3/10 | Train Loss: 0.3776, Acc: 0.6835 | Val Loss: 0.3886, Acc: 0.6654\n",
      "Epoch 4/10 | Train Loss: 0.3777, Acc: 0.6826 | Val Loss: 0.4106, Acc: 0.6654\n",
      "Epoch 5/10 | Train Loss: 0.3658, Acc: 0.6864 | Val Loss: 0.3927, Acc: 0.6654\n",
      "Epoch 6/10 | Train Loss: 0.3629, Acc: 0.6845 | Val Loss: 0.3822, Acc: 0.6654\n",
      "Epoch 7/10 | Train Loss: 0.3619, Acc: 0.6873 | Val Loss: 0.3681, Acc: 0.6654\n",
      "Epoch 8/10 | Train Loss: 0.3298, Acc: 0.7550 | Val Loss: 0.2759, Acc: 0.8175\n",
      "Epoch 9/10 | Train Loss: 0.2417, Acc: 0.8684 | Val Loss: 0.2565, Acc: 0.8859\n",
      "Epoch 10/10 | Train Loss: 0.2263, Acc: 0.9056 | Val Loss: 0.2371, Acc: 0.9011\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, true_labels = torch.max(labels, 1)\n",
    "        correct += (predicted == true_labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_acc.append(train_accuracy)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, true_labels = torch.max(labels, 1)\n",
    "            val_correct += (predicted == true_labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_acc.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Acc: {train_accuracy:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Acc: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08c0261d-1483-4f8c-bf68-1b432cb429cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       175\n",
      "           1       0.96      0.74      0.83        88\n",
      "\n",
      "    accuracy                           0.90       263\n",
      "   macro avg       0.92      0.86      0.88       263\n",
      "weighted avg       0.91      0.90      0.90       263\n",
      "\n",
      "Confusion Matrix:\n",
      " [[172   3]\n",
      " [ 23  65]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        _, targets = torch.max(labels, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(all_targets, all_preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(all_targets, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83e73402-bc3c-40c2-bb99-5288d098cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_loader, test_loader, epochs=10, lr=0.0001):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, true_labels = torch.max(labels, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(true_labels.cpu().numpy())\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds),\n",
    "        'recall': recall_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds),\n",
    "        'mcc': matthews_corrcoef(all_labels, all_preds)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f38013cb-a4b6-40a1-b08a-8bf5719b480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all models\n",
    "def get_models():\n",
    "    return {\n",
    "        \"SqueezeNet\": models.squeezenet1_1(pretrained=True),\n",
    "        \"ResNet50\": models.resnet50(pretrained=True),\n",
    "        \"VGG16\": models.vgg16(pretrained=True),\n",
    "        \"MobileNetV2\": models.mobilenet_v2(pretrained=True),\n",
    "        \"DenseNet121\": models.densenet121(pretrained=True)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50547068-0c38-4389-9257-cf06f6933711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_loader, test_loader, epochs=10, lr=0.0001):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_acc_list, val_acc_list = [], []\n",
    "    train_loss_list, val_loss_list = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, targets = torch.max(labels, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, targets = torch.max(labels, 1)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Acc: {train_accuracy:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Final evaluation metrics\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, targets = torch.max(labels, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds),\n",
    "        'recall': recall_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds),\n",
    "        'mcc': matthews_corrcoef(all_labels, all_preds),\n",
    "        'train_acc': train_acc_list,\n",
    "        'val_acc': val_acc_list,\n",
    "        'train_loss': train_loss_list,\n",
    "        'val_loss': val_loss_list\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f7589d2-3f7d-4c6a-873e-6e92be47c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model_for_binary(model, model_name):\n",
    "    if model_name == \"SqueezeNet\":\n",
    "        # Replacing classifier with 2-class head\n",
    "        model.classifier[1] = nn.Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "        model.num_classes = 2\n",
    "\n",
    "    elif model_name == \"ResNet50\":\n",
    "        # ResNet uses `fc` as final classifier\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, 2)\n",
    "\n",
    "    elif model_name == \"DenseNet121\":\n",
    "        # DenseNet uses `classifier`\n",
    "        in_features = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(in_features, 2)\n",
    "\n",
    "    elif model_name == \"VGG16\":\n",
    "        # VGG classifier is a sequential with [0-6]\n",
    "        in_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(in_features, 2)\n",
    "\n",
    "    elif model_name == \"MobileNetV2\":\n",
    "        # MobileNetV2 classifier is sequential with 2 layers\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, 2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not supported\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e34154-a835-46dd-a5f1-3a9401ef6736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CVR\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\CVR\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\CVR\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\CVR\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\CVR\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\CVR\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SqueezeNet...\n",
      "Epoch 1/10 | Train Loss: 0.4601, Acc: 0.7712 | Val Loss: 0.3427, Acc: 0.8441\n",
      "Epoch 2/10 | Train Loss: 0.2790, Acc: 0.8742 | Val Loss: 0.2587, Acc: 0.8935\n",
      "Epoch 3/10 | Train Loss: 0.2323, Acc: 0.9037 | Val Loss: 0.2409, Acc: 0.8783\n",
      "Epoch 4/10 | Train Loss: 0.1786, Acc: 0.9199 | Val Loss: 0.2378, Acc: 0.9125\n",
      "Epoch 5/10 | Train Loss: 0.2216, Acc: 0.8951 | Val Loss: 0.2468, Acc: 0.8935\n",
      "Epoch 6/10 | Train Loss: 0.1488, Acc: 0.9418 | Val Loss: 0.2324, Acc: 0.9278\n",
      "Epoch 7/10 | Train Loss: 0.1165, Acc: 0.9523 | Val Loss: 0.1715, Acc: 0.9316\n",
      "Epoch 8/10 | Train Loss: 0.1423, Acc: 0.9390 | Val Loss: 0.2281, Acc: 0.9011\n",
      "Epoch 9/10 | Train Loss: 0.1172, Acc: 0.9523 | Val Loss: 0.2020, Acc: 0.9163\n",
      "Epoch 10/10 | Train Loss: 0.0865, Acc: 0.9647 | Val Loss: 0.1642, Acc: 0.9430\n",
      "\n",
      "Training ResNet50...\n",
      "Epoch 1/10 | Train Loss: 0.3557, Acc: 0.8246 | Val Loss: 0.3384, Acc: 0.8175\n",
      "Epoch 2/10 | Train Loss: 0.1794, Acc: 0.9361 | Val Loss: 0.2742, Acc: 0.8783\n",
      "Epoch 3/10 | Train Loss: 0.1364, Acc: 0.9533 | Val Loss: 0.1452, Acc: 0.9544\n",
      "Epoch 4/10 | Train Loss: 0.0760, Acc: 0.9762 | Val Loss: 0.1850, Acc: 0.9125\n",
      "Epoch 5/10 | Train Loss: 0.0499, Acc: 0.9838 | Val Loss: 0.1414, Acc: 0.9468\n",
      "Epoch 6/10 | Train Loss: 0.0498, Acc: 0.9847 | Val Loss: 0.1508, Acc: 0.9392\n",
      "Epoch 7/10 | Train Loss: 0.0531, Acc: 0.9828 | Val Loss: 0.3110, Acc: 0.8555\n",
      "Epoch 8/10 | Train Loss: 0.0622, Acc: 0.9847 | Val Loss: 0.1389, Acc: 0.9582\n",
      "Epoch 9/10 | Train Loss: 0.0262, Acc: 0.9914 | Val Loss: 0.1391, Acc: 0.9506\n",
      "Epoch 10/10 | Train Loss: 0.0447, Acc: 0.9886 | Val Loss: 0.3176, Acc: 0.9240\n",
      "\n",
      "Training VGG16...\n",
      "Epoch 1/10 | Train Loss: 0.3392, Acc: 0.8313 | Val Loss: 0.3107, Acc: 0.8707\n",
      "Epoch 2/10 | Train Loss: 0.2438, Acc: 0.9018 | Val Loss: 0.2412, Acc: 0.9163\n",
      "Epoch 3/10 | Train Loss: 0.1904, Acc: 0.9113 | Val Loss: 0.2355, Acc: 0.9087\n",
      "Epoch 4/10 | Train Loss: 0.1228, Acc: 0.9476 | Val Loss: 0.2888, Acc: 0.9087\n",
      "Epoch 5/10 | Train Loss: 0.1224, Acc: 0.9542 | Val Loss: 0.3361, Acc: 0.8327\n",
      "Epoch 6/10 | Train Loss: 0.1188, Acc: 0.9600 | Val Loss: 0.2815, Acc: 0.8897\n",
      "Epoch 7/10 | Train Loss: 0.0841, Acc: 0.9695 | Val Loss: 0.3619, Acc: 0.8745\n",
      "Epoch 8/10 | Train Loss: 0.0535, Acc: 0.9771 | Val Loss: 0.3022, Acc: 0.9316\n",
      "Epoch 9/10 | Train Loss: 0.0375, Acc: 0.9905 | Val Loss: 0.3894, Acc: 0.9240\n",
      "Epoch 10/10 | Train Loss: 0.0337, Acc: 0.9914 | Val Loss: 0.3187, Acc: 0.9468\n",
      "\n",
      "Training MobileNetV2...\n",
      "Epoch 1/10 | Train Loss: 0.3997, Acc: 0.8151 | Val Loss: 0.2329, Acc: 0.9011\n",
      "Epoch 2/10 | Train Loss: 0.1887, Acc: 0.9171 | Val Loss: 0.1598, Acc: 0.9202\n",
      "Epoch 3/10 | Train Loss: 0.1105, Acc: 0.9628 | Val Loss: 0.1685, Acc: 0.9202\n",
      "Epoch 4/10 | Train Loss: 0.0858, Acc: 0.9743 | Val Loss: 0.2223, Acc: 0.9125\n",
      "Epoch 5/10 | Train Loss: 0.0713, Acc: 0.9762 | Val Loss: 0.1804, Acc: 0.9240\n",
      "Epoch 6/10 | Train Loss: 0.0476, Acc: 0.9905 | Val Loss: 0.1328, Acc: 0.9544\n",
      "Epoch 7/10 | Train Loss: 0.0550, Acc: 0.9809 | Val Loss: 0.2901, Acc: 0.9011\n",
      "Epoch 8/10 | Train Loss: 0.0334, Acc: 0.9876 | Val Loss: 0.1611, Acc: 0.9582\n",
      "Epoch 9/10 | Train Loss: 0.0348, Acc: 0.9876 | Val Loss: 0.1259, Acc: 0.9620\n",
      "Epoch 10/10 | Train Loss: 0.0270, Acc: 0.9933 | Val Loss: 0.1231, Acc: 0.9468\n",
      "\n",
      "Training DenseNet121...\n",
      "Epoch 1/10 | Train Loss: 0.3191, Acc: 0.8713 | Val Loss: 0.2250, Acc: 0.8935\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "models_dict = get_models()\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model = modify_model_for_binary(model, name)\n",
    "    metrics = train_and_evaluate_model(model, train_loader, test_loader, epochs=10, lr=0.0001)\n",
    "    results[name] = metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b547f-c749-4339-9c3b-a2edceeb52d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(results, metric_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for model_name, metrics in results.items():\n",
    "        plt.plot(metrics[metric_name], label=model_name)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name.replace('_', ' ').title())\n",
    "    plt.title(f\"{metric_name.replace('_', ' ').title()} per Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(results, 'train_acc')\n",
    "plot_metrics(results, 'val_acc')\n",
    "plot_metrics(results, 'train_loss')\n",
    "plot_metrics(results, 'val_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69bb66-cc50-40f4-8402-123f460fa61b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
